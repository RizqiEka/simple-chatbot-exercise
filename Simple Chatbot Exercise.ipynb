{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MAIN CODE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day', 'Whats up'], 'responses': ['Hello!', 'Good to see you again!', 'Hi there, how can I help?'], 'context_set': ''}, {'tag': 'goodbye', 'patterns': ['cya', 'See you later', 'Goodbye', 'I am Leaving', 'Have a Good day'], 'responses': ['Sad to see you go :(', 'Talk to you later', 'Goodbye!'], 'context_set': ''}, {'tag': 'age', 'patterns': ['how old', 'how old is tim', 'what is your age', 'how old are you', 'age?'], 'responses': ['I am 18 years old!', '18 years young!'], 'context_set': ''}, {'tag': 'name', 'patterns': ['what is your name', 'what should I call you', 'whats your name?'], 'responses': ['You can call me Tim.', \"I'm Tim!\", \"I'm Tim aka Tech With Tim.\"], 'context_set': ''}, {'tag': 'shop', 'patterns': ['Id like to buy something', 'whats on the menu', 'what do you reccommend?', 'could i get something to eat'], 'responses': ['We sell chocolate chip cookies for $2!', 'Cookies are on the menu!'], 'context_set': ''}, {'tag': 'hours', 'patterns': ['when are you guys open', 'what are your hours', 'hours of operation'], 'responses': ['We are open 7am-4pm Monday-Friday!'], 'context_set': ''}]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day', 'Whats up'], 'responses': ['Hello!', 'Good to see you again!', 'Hi there, how can I help?'], 'context_set': ''}, {'tag': 'goodbye', 'patterns': ['cya', 'See you later', 'Goodbye', 'I am Leaving', 'Have a Good day'], 'responses': ['Sad to see you go :(', 'Talk to you later', 'Goodbye!'], 'context_set': ''}, {'tag': 'age', 'patterns': ['how old', 'how old is tim', 'what is your age', 'how old are you', 'age?'], 'responses': ['I am 18 years old!', '18 years young!'], 'context_set': ''}, {'tag': 'name', 'patterns': ['what is your name', 'what should I call you', 'whats your name?'], 'responses': ['You can call me Tim.', \"I'm Tim!\", \"I'm Tim aka Tech With Tim.\"], 'context_set': ''}, {'tag': 'shop', 'patterns': ['Id like to buy something', 'whats on the menu', 'what do you reccommend?', 'could i get something to eat'], 'responses': ['We sell chocolate chip cookies for $2!', 'Cookies are on the menu!'], 'context_set': ''}, {'tag': 'hours', 'patterns': ['when are you guys open', 'what are your hours', 'hours of operation'], 'responses': ['We are open 7am-4pm Monday-Friday!'], 'context_set': ''}]\n"
     ]
    }
   ],
   "source": [
    "print(data['intents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent['tag'])\n",
    "\n",
    "        if intent['tag'] not in labels:\n",
    "            labels.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['greeting', 'goodbye', 'age', 'name', 'shop', 'hours']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "words = sorted(list(set(words)))\n",
    "labels = sorted(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    wrds = [stemmer.stem(w) for w in doc]\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    \n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "\n",
    "training = np.array(training)\n",
    "output = np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.02833\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1000 | loss: 0.02833 - acc: 0.9999 -- iter: 24/26\n",
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.02779\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1000 | loss: 0.02779 - acc: 0.9999 -- iter: 26/26\n",
      "--\n",
      "INFO:tensorflow:d:\\Personal Data\\Data Science\\Personal Project\\February\\Chatbot\\simplecb.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save(\"simplecb.tflearn\")"
   ]
  },
  {
   "source": [
    "## Phase 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"simplecbdata.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "\n",
    "except:\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent['tag'])\n",
    "\n",
    "            if intent['tag'] not in labels:\n",
    "                labels.append(intent['tag'])\n",
    "\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [stemmer.stem(w) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "        \n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "training = np.array(training)\n",
    "output = np.array(output)\n",
    "\n",
    "with open(\"simplecbdata.pickle\", \"wb\") as f:\n",
    "    pickle.dump((words, labels, training, output), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Restoring parameters from d:\\Personal Data\\Data Science\\Personal Project\\February\\Chatbot\\simplecb.tflearn\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "try:\n",
    "    model.load(\"simplecb.tflearn\")\n",
    "except:\n",
    "    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "    model.save(\"simplecb.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start talking with the bot (type 'quit' to stop)!\n",
      "Hi there, how can I help?\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for j in s_words:\n",
    "        for k, l in enumerate(words):\n",
    "            if l == j:\n",
    "                bag[k] = 1\n",
    "\n",
    "    return np.array(bag)\n",
    "\n",
    "def chat():\n",
    "    print(\"Start talking with the bot (type 'quit' to stop)!\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "        \n",
    "        results = model.predict([bag_of_words(inp, words)])\n",
    "        results_index = np.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        for tg in data['intents']:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "            \n",
    "        print(random.choice(responses))\n",
    "\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start talking with the bot (type 'quit' to stop)!\n",
      "I didn't get that, try again\n",
      "I didn't get that, try again\n",
      "I didn't get that, try again\n",
      "I didn't get that, try again\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for j in s_words:\n",
    "        for k, l in enumerate(words):\n",
    "            if l == j:\n",
    "                bag[k] = 1\n",
    "\n",
    "    return np.array(bag)\n",
    "\n",
    "def chat():\n",
    "    print(\"Start talking with the bot (type 'quit' to stop)!\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "        \n",
    "        results = model.predict([bag_of_words(inp, words)])[0]\n",
    "        results_index = np.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        if results[results_index] > 0.99:\n",
    "            for tg in data['intents']:\n",
    "                if tg['tag'] == tag:\n",
    "                    responses = tg['responses']\n",
    "            print(random.choice(responses))\n",
    "\n",
    "        else:\n",
    "            print(\"I didn't get that, try again\")\n",
    "\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}